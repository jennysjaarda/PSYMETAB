---
title: "Genetic data quality control and processing"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
---

```{css include=FALSE}
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
options(width = 150)
```

**The following document outlines and summarizes the genetic quality control and processing procedure that was followed to create a clean, imputed dataset.**
	
# Step 1: Prepare and cluster genomestudio files.
	
*Step 1 was performed entirely on CHUV computer*
	
## Part A: Randomize IDs.
	
- Genetic sampleIDs were recoded according to GPCR algorithm to ensure genetic participants are not identifiable.
- `code/radomize_IDs.r` was run on CHUV computer before building GenomeStudio project.
- Creates a new csv file which was used to create a GenomeStudio project with data provided by lab in Geneva.
- Requires manual addition of header before uploading to GenomeStudio.

```{r eval=FALSE}

[Header],,,,,,,,,,,,,
Investigator Name,,,,,,,,,,,,,
Project Name,,,,,,,,,,,,,
Experiment Name,,,,,,,,,,,,,
Date,,,,,,,,,,,,,
[Manifests],,,,,,,,,,,,,
A,GSA_UPPC_20023490X357589_A1,,,,,,,,,,,,
[Data],,,,,,,,,,,,,

```

- Some samples were found to be duplicates (i.e. 2 samples at 2 different time points were analyzed for the same individual) and they were recoded to have ID as: `${ID}002`.

## Part B: Create GenomeStudio files.


- Instructions can be found [here](https://support.illumina.com/content/dam/illumina-support/documents/documentation/software_documentation/genomestudio/genomestudio-2-0/genomestudio-genotyping-module-v2-user-guide-11319113-01.pdf).
- Required files:
  - Sample sheet: as csv file (created above).
  - Data repository: as idat files.
  - Manifest file: as bpm file.
  - Cluster file: as egt file.
- Data provided from Mylene Docquier, copied from sftp and saved here:  `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\data`.
- Create new IDs based on GPCR randomization (see `code/randomize_IDs.r`), and save to above folder as: `Eap0819_1t26_27to29corrected_7b9b_randomizedID.csv`.
- Note that original IDs can be found in the same folder at the file: `Eap0819_1t26_27to29corrected_7b9.csv`, if needed.
- Create empty folder here: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS`, named: `GS_project_26092019` (data of creation).
- Using new IDs, create genome studio project as follows:
  1. Open GenomeStudio.
  2. Select: File > New Genotyping Project.
  3. Select `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS` as project repository.
  4. Under project name: use "GS_project_26092019" and click "Next".
  5. Select "Use sample sheet to load intensities" and click "Next".
  6. Select sample, data and manifests as specified below and click "Next":
     - Sample sheet:  `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\data\Eap0819_1t26_27to29corrected_7b9b_randomizedID.csv`,
     - Data repository: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\data`,
     - Manifest repository: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\data`.
  7. Select "Import cluster positions from cluster file" and choose cluster file located here: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\data\GSPMA24v1_0-A_4349HNR_Samples.egt` and click "Finish".

- Genome studio files were created using the following required files:
  - Sample sheet: as csv file (created above).
  - Data repository: as idat files (provided from Mylene Docquier).
  - Manifest file: as bpm file (provided from Mylene Docquier).
  - Cluster file: as egt file (provided from Mylene Docquier).

## Part C: Clustering and PLINK conversion.

- Data saved on CHUV servers at the following location: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\GS_project_26092019`.
- `GS_project_26092019.bsc` was opened (requires Genome Studio) and used for clustering.
- Clustering was performed according to the guidelines in the webinar (see [notes on creating custom cluster files](create_custom_cluster_files.html)) with the following procedure:
  1. **Cluster**: cluster all SNPs, evaluate samples.
     1. Once data was opened, all SNPs were clustered by clicking "Cluster all SNPs button" in the top panel (icon with 3 red, purple, blue ovals). SNP statistics and heritability estimate updates were ignored.
     2. In the samples table (bottom left panel) call rate was recalculated by clicking "Calculate" (calculator icon) to recalculate SNP call rates with new cluster positions. SNP statistics and heritability estimate updates were ignored.
     3. The sample table was then sorted by the "Call Rate" column and samples with call rate <95% were selected and removed from downstream processing (right click, select "Exclude Selected Samples"), 8 samples fell below this cut-off. Updates were ignored. In "SNP Graph" right click and deselect "Show Excluded Samples".
  2. **Recluster**: cluster sex chromosomes, cluster autosomes.
     1. "SNP Table" was filtered by ("Chr" = Y) using filter icon.
     2. Using the 3rd and 7th SNP (index 3010 and 3014, 3 samples had missing values for 3010 and sex was determined to be female using 3014), males were selected as those with high intensities (females have no Y chromosome) and set to have an aux value of 101 by right clicking the selected samples in the "Samples Table". Similarly, females were selected and set to have an aux value of 102.
     3. "Samples Table" was filtered to have ("Aux" > 100).
     4. "Samples Table" was sorted on "Aux" column and females were selected (value 102) and excluded. Updates were ignored.
     5. In "SNP Table", all SNPs were selected and Y-snps were clustered (right click and "Cluster Selected SNPs") on only male samples. Updates were ignored.
     6. Females were re-added to the project in the "Samples Table".
     7. "SNP Table" was filtered by ("Chr" = X) using filter icon.
     8. In "Samples Table", males were selected (Aux value 101) and excluded. Updates were ignored.
     9. In "SNP Table", all SNPs were selected and X-SNPs were clustered (right click and "Cluster Selected SNPs") on only female samples. Updates were ignored.
     10. Males were re-added to the project in the "Samples Table".
     11. "SNP Table" was filtered by [ !("Chr" = X ) AND !("Chr" = Y ) ], using filter icon.
     12. In "SNP Table", all SNPs were selected and autosomal SNPs were clustered (right click and "Cluster Selected SNPs") on all good quality samples. Update SNP statistics.
     13. In the samples table (bottom left panel) call rate was recalculated by clicking "Calculate" (calculator icon) to recalculate SNP call rates with new cluster positions. SNP statistics and heritability estimate updates were ignored.
     14. SNP statistics were updated ("Analysis" > "Update SNP statistics").
  3. **Review and edit**: use filters and scores to evaluate SNPs, correct or zero SNPs as needed. No manual editing was performed.
- New genome studio was exported to the following location: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS`, and named:  `GS_project_26092019_cluster`.
- Remove filtered individuals:
  - In samples table, remove filter for "Aux" > 100.
  - Recalculate SNP statistics for these 8 samples only (to save time).
  - Note that sample call rates have increased with new clustering positions.
- Project was exported as PLINK file to the following location: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS`, and named: `PLINK_091019_0920`.
- Individual PLINK files within above folder were named according to parent directory as: PSYMETAB_GWAS.ped and PSYMETAB_GWAS.map.
- Ultimately, the final ped/map files reside here: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\PLINK_091019_0920`.

### Notes on GenomeStudio.
- Processing data within GS software is *slow* in comparison to the rest of the pipeline.
- There is no command line version, only a licensed, GUI version available.
- It is impossible to fully automate and/or parallelize.

### Data updates and releases.
- Initial data was received on April 8, 2019:
  - Final two plates were received on May 17, 2019.
  - Processing began with initial files.
- July 18, 2019 update:
  - It came to our attention that 15 participants were genotyped that did not consent.
  - This list was sent to Mylene to be removed.
  - Plates 27 to 29 were re-provided on 08/08/2019 without these 15 individuals (list below, and provided by Severine in email).
```{r eval=FALSE}
013CB
017CB
074CB
095CB
150CRV
192CRV
193CRV
156CSM
181CSM
191CSM
224UAS
234GL
058GP
246GP
089PP
```
  - Until new file was provided, these participants were removed using PLINK to avoid any further analysis of these individuals.
  - The new genomestudio file was copied to: `L:\PCN\UBPC\ANALYSES_RECHERCHE\Jenny\PSYMETAB_GWAS\PSYMETAB_GS2\Plates27to29_0819`.
  - The same process above was followed (data opened in GS, cluster positions imported, and data saved to `Plates27to29_0819_cluster`, and `PLINK_270819_0457`).
  - Old files were deleted to remove all data containing these individuals.
  - Updated files were then copied to PSYMETAB_GS1.
- August 28, 2019 update:
  - Mylene provided one single genome studio file with all samples (excluding the list from Severine).
  - These files were copied to UPPC folders and custom clustering was re-performed.
  - These changes are reflected in the above description.
  - All old files were subsequently deleted to ensure the data from these participants is completely removed from all databases.
  - As of September 3, 2019, all clustering was complete and final PLINK files (`PLINK_030919_0149`) were copied to SGG directory (names of plink files according to parent directory: `DATA`).
- September 6, 2019 update:
  - It was decided that all IDs part of PSYMETAB should be randomized to ensure they are not identifiable.
  - We had a meeting to discuss (Celine, Fred, Chin, Nermine, and Claire), and decided to use a CHUV program (GPCR) for the randomization process.
  - We requested with Mylene to create a new project with the new IDs, but she suggested to create our own GS project.
  - She provided all relevant data to create our own GS project.
  - The description above reflects these changes.
- As of October 11, all GS file were created, clustered and exported as PLINK files and subsequently moved to the sgg server.

## Part D: Copy files to SGG server.
- PLINK files were copied to SGG servers using FileZilla
- Host name: `je4649@hpc1.chuv.ch`
- Password: `<chuv-password>`
- Port: `22`
- Output saved to: `/data/sgg2/jenny/projects/PSYMETAB_GWAS/data/raw`.

*All subsequent steps were performed on the `sgg` server and run using `drake` plan*


# Step 2: Pre-quality control data prep.

**See `qc_prep` drake plan in [code/plans.sh](https://github.com/jennysjaarda/PSYMETAB/blob/master/code/plan.R).**  

- Processed sex and ethnicity files to be used in QC scripts.
- Sex file was created according to the input specified on plink man page (FID, IID, sex [M/F]).
- Ethnicity input file to be used in R script for comparison to genetically derived ethnic groups (by snpweights).
- Recodes ethnic groups as follows:
  - Changes French codes to English.
  - Changes missing to unknown.
  - Groups small ethnic groups to missing.
- A1 rsid conversion file was updated to remove all SNPs labeled with a [.] (see [data sources](data_sources.html)).
- Create duplicates file

# Step 3: Pre-imputation quality control.

*Results of Step 3-6 are saved to `analysis/QC`. The majority of analyses were performed using [`PLINK`](https://www.cog-genomics.org/plink/2.0/) (either version 2.0 or 1.9)*
*Each sub-spet (i.e. 0-15) corresponds to one folder within `analysis/QC`*

**Source code for Step 3 can be found at: [code/pre_imputation_qc.sh](https://github.com/jennysjaarda/PSYMETAB/blob/master/code/pre_imputation_qc.sh).**   

```{r setup, include = FALSE}
options(scipen=999)
source("code/packages.R")
source("code/functions.R")
source("code/settings.R")


library(R.utils)
library(kableExtra)
project_dir <- "/data/sgg2/jenny/projects/PSYMETAB"
output_name <- "PSYMETAB_GWAS"
input_chip <-plink_bed_out
sex_file <- read.table('data/processed/phenotype_data/PSYMETAB_GWAS_sex.txt', header=F)
loadd(dups)
```


```{r missingness_vars, include = FALSE}
missingness_variants_removed <- countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_10.bim"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_10.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_05.bim"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_05.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_01.bim"))[1]
    
missingness_indiv_removed <- countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_10.fam"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_05.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_05.fam"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_01.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness.step3.fam"))[1]
    
initial_variants <- countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.bim"))[1]
initial_ind <- countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.fam"))[1]
```

## 0. Preprocessing.

1. Create binary PLINK files (if necessary):
    - `r countLines(paste0(input_chip, ".bim"))` variants initially.
    - `r countLines(paste0(input_chip, ".fam"))` individuals initially.
2. Exclude Y and MT variants:
    - `r countLines(paste0("analysis/QC/00_preprocessing/non_autosome_or_x_variants"))` Y / MT variants removed.
    - `r countLines(paste0("analysis/QC/00_preprocessing/temp1.bim"))` variants remaining.
3. Remove duplicate variants:
    - `r countLines(paste0("analysis/QC/00_preprocessing/duplicate_variants"))` duplicates removed.
    - `r countLines(paste0("analysis/QC/00_preprocessing/temp2.bim"))` variants remaining.
4. Update sex:
    - Using the file located at: `data/processed/phenotype_data/PSYMETAB_GWAS_sex.txt` (created above).
    ```{r init_sex, echo = FALSE}
    table(sex_file[,3])
    ```
5. Remove duplicate individuals that were identified previously (i.e. two different IDs for the same participant):
    - `r length(dups$dups)` duplicates identified and removed
    - `r countLines(paste0("analysis/QC/00_preprocessing/", study_name, ".preprocessing.step0.fam"))` indviduals remaining.
6. Sanity check: ensure that duplicates have the same genetic info"
    - From the PLINK [webpage](https://www.cog-genomics.org/plink/2.0/distance#make_king): *Note that KING kinship coefficients are scaled such that duplicate samples have kinship 0.5, not 1. First-degree relations (parent-child, full siblings) correspond to ~0.25, second-degree relations correspond to ~0.125, etc.*
    - If all duplicate ID samples have KINSHIP ~0.5, then indeed they are genetic duplicates. 
        
```{r duplicates, echo = FALSE}
t <- read.table(  paste0("analysis/QC/00_preprocessing/", study_name, ".duplicates.kin0"))
colnames(t) <- c('#FID1', 'ID1', 'FID2', 'ID2', 'NSNP', 'HETHET', 'IBS0', 'KINSHIP')
t
```


\newline

## 1. Strand alignment (so all SNPs are on positive strand).

1. Update chromosome
2. Update position
3. Flip alleles on negative strand
4. Extract SNPs that are not in strand file
5. Remove any non autosomal / X chromosomal variants (might have changed due to the pos and chr update):
    - `r countLines(paste0("analysis/QC/01_strandflip/non_autosome_or_x_variants"))` duplicates removed.
    - `r countLines(paste0("analysis/QC/01_strandflip/temp5.bim"))` variants remaining.
6. Update rsids using `data/processed/reference_files/rsid_conversion.txt`
7. Remove duplicate variants:
    - `r countLines(paste0("analysis/QC/01_strandflip/duplicate_variants"))` duplicates removed.
    - `r countLines(paste0("analysis/QC/01_strandflip/", study_name, ".strandflip.step1.bim"))` variants remaining.

\newline

## 2. Removal of SNPs that have MAF zero.

1. Calculate frequency of all SNPs
2. Remove MAF 0 SNPs: 
    - `r countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero_snp.txt"))` variants with `MAF = 0`.
    - `r countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.bim"))` variants remaining.

\newline

## 3. Missingness.

1. Exclude variants with >10% missingness (using `geno --0.1`): 
    - `r countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_10.bim"))[1]` variants removed.
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_10.bim"))` variants remaining.
2. Exclude individuals with >10% missingness (using `mind --0.1`): 
    - `r countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_10.fam"))[1]` individuals removed.
      - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_10.fam"))` individuals remaining.
3. Exclude variants with >5% missingness (using `geno --0.05`): 
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_10.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_05.bim"))[1]` variants removed.
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_05.bim"))` variants remaining.
4. Exclude individuals with >5% missingness (using `mind --0.05`): 
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_05.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_05.fam"))[1]` individuals removed.
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_05.fam"))` individuals remaining.
5. Exclude variants with >1% missingness (using `geno --0.01`): 
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_05.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_01.bim"))[1]` variants removed.
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_01.bim"))` variants remaining.
6. Exclude individuals with >1% missingness (using `mind --0.01`): 
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_01.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness.step3.fam"))[1]` individuals removed.
    - `r countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness.step3.fam"))` individuals remaining.
        
\newline
**Total removed: `r countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_10.bim"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_10.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_05.bim"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_05.bim"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_01.bim"))[1]` variants (`r formatC(round(missingness_variants_removed*100/initial_variants, 2), format='f', digits=2)`%) and `r countLines(paste0("analysis/QC/02_maf_zero/", study_name, ".maf_zero.step2.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_10.fam"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_05.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_mind_05.fam"))[1] + countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness_geno_01.fam"))[1] - countLines(paste0("analysis/QC/03_missingness/", study_name, ".missingness.step3.fam"))[1]` individuals (`r formatC(round(missingness_indiv_removed*100/initial_ind, 2), format='f', digits=2)`%).**

\newline

## 4. Sex check.

1. Perform sex check. 
2. Remove unambiguous sex violations. 
    - `r countLines(paste0("analysis/QC/04_sexcheck/", study_name, ".sexcheck.problem.ids"))[1]` individuals removed.
    - `r countLines(paste0("analysis/QC/04_sexcheck/", study_name, ".sexcheck.step4.fam"))` individuals remaining.
    
## 5. Imputation preparation.

1. Write frequency of final QC'd file (from #4) to file (using `-- freq`).
2. Using [McCarthy Group Tools](https://www.well.ox.ac.uk/~wrayner/tools/), QC'd files were prepared for imputation using the script [`HRC-1000G-check-bim-NoReadKey.pl`](https://www.well.ox.ac.uk/~wrayner/tools/HRC-1000G-check-bim-v4.2.11-NoReadKey.zip) (download link).
    - This script checks: Strand, alleles, position, Ref/Alt assignments and frequency differences.
    - Produces: A set of plink commands to update or remove SNPs based on the checks as well as a file (FreqPlot) of cohort allele frequency vs reference panel allele frequency.
    - Updates: Strand, position, ref/alt assignment.
    - Removes: A/T & G/C SNPs if MAF > 0.4, SNPs with differing alleles, SNPs with > 0.2 allele frequency difference (can be removed/changed in V4.2.2), SNPs not in reference panel
3. Replace underscores in fam file since vcf conversion uses understcores between `FID` and `IID`.
4. Run the generated `Run-plink.sh` script from #2.
5. Sort outputed `vcf` files.
6. Download zipped files to personl or CHUV files to copy to [Michigan Imputation Server](https://imputationserver.sph.umich.edu/index.html).

\newline

# Step 4: Imputation.

**Source code for Step 4 can be found at: [code/download_imputation.sh](https://github.com/jennysjaarda/PSYMETAB/blob/master/code/pre_imputation_qc.sh) and [code/check_imputation.sh](https://github.com/jennysjaarda/PSYMETAB/blob/master/code/check_imputation.sh).**

## 6. Run and download imputation.

1. Upload downloaded QC'd `vcf.gz` files to [Michigan Imputation Server](https://imputationserver.sph.umich.edu/index.html) as follows:
    - Select `Run`, `Genotype Imputation (Minimac4)`.
    - Reference panel: `HRC r1.1 2016 (GRCh37/hg19)`.
    - Array build: `GRCh37/hg19`.
    - rsq Filter: `off`.
    - Phasing: `Eagle v2.4 (phased output)`.
    - Population: `EUR`.
    - Mode: `Quality Control & Imputation`.
  ![](assets/michigan_imputation_screenshot_03022020.PNG)

\newline
2. Download imputation, using password from email retrieve the following files:
    - [QC report](generated_reports/qcreport.html).
    - QC stats.
    - Logs.
    - Imputation results.
\newline

```{r imputed_variants, echo = FALSE}

list.files("analysis/QC//06_imputation_get/")

imputed_var <- numeric()
for (chr in 1:22){

  temp <- countLines(paste0("analysis/QC/06_imputation_get/chr", chr, ".info.gz"))[1] - 1
  temp <- cbind(chr, temp)
  imputed_var <- rbind(imputed_var, temp)

}
total <- cbind("all", sum(imputed_var[,2]))
imputed_var <- rbind(imputed_var, total)
colnames(imputed_var) <- c("Chr", "Num imputed variants")
imputed_var <- as.data.frame(imputed_var)
imputed_var
```

\newline

## 7. Check imputation.

- Using Will Rayner's [post imputation tool](https://www.well.ox.ac.uk/~wrayner/tools/Post-Imputation.html), produce a visual summary of the imputated results from the Michigan Imputation Server:
    - [Imputation report](generated_reports/07_imputation_check.html).
    - [Cross cohort report](generated_reports/CrossCohortReport.html).

\newline

# Step 5: Post imputation quality control.

## 8. PLINK conversion.

**Source code for Step 5 can be found at: [code/post_imputation_qc.sh](https://github.com/jennysjaarda/PSYMETAB/blob/master/code/post_imputation_qc.sh).**

1. Remove SNPs with `info < 0.30`
```{r filtered_imputed_var, echo = FALSE}

imputed_var[, "R2 filtered"] <- NA
for (chr in 1:22){

  temp <- countLines(paste0("analysis/QC/08_plink_convert/chr", chr, "/chr", chr, ".pvar"))[1] - 14
  imputed_var[["R2 filtered"]][chr] <- temp

}
total_temp <- sum(imputed_var[1:22,3])
imputed_var[["R2 filtered"]][23] <- total_temp
imputed_var
```

2. Update `bim` file to include rsIDs instead of `chr:bp` convention:
    - Output of Michigan Impuation server is  in format `chr:bp:ref:alt`.
    - Convenient to have SNPs in regular rsIDs for extraction, etc.
    - If there is no known rsID, SNP name is left as `chr:bp:ref:alt`.
    - Reference file for determining rsIDs can be found here: `/data/sgg2/jenny/data/dbSNP/dbSNP_SNP_list_chr${chr}.txt`, which was processed according to description in `jenny/SGG_generic/scripts/public_data.sh`.

\newline

## 9. Extract typed SNPs.

1. Write a list of only genotypd data to run in snpweights software (using `require=info "TYPED"` flag).
```{r typed_snps, echo = FALSE}

imputed_var[, "Typed SNPs"] <- NA
for (chr in 1:22){

  temp <- countLines(paste0("analysis/QC/09_extract_typed/chr", chr, "/temp1.pvar"))[1] - 14
  imputed_var[["Typed SNPs"]][chr] <- temp

}
total_temp <- sum(imputed_var[1:22,4])
imputed_var[["Typed SNPs"]][23] <- total_temp
imputed_var
```
2. Hard call back to genotypes (`bim`, `bed`, and `fam` files) using the `--hard-call-threshold` flag set at `0.1`.
3. Merge by chromosome genotyped data into single file.

\newline

## 10. Merge imputed SNPs.

1. Convert fileset from step #8 back to `vcf` as there is no merge function in plink (using the flag `--recode vcf id-paste=iid vcf-dosage=HDS`).
2. Merge using `bcftools concat`.
3. Convert back to pgen using `plink2 --vcf <output-name> dosage=HDS`.

\newline

## 11. Relatedness (using data from #10).

1. Calculate missingness.
2. Remove subjects and individuals with missingness >10%.
    - `r countLines(paste0("analysis/QC/11_relatedness/", study_name, ".filter.vmiss"))[1] - 1` variants removed.
    - `r countLines(paste0("analysis/QC/11_relatedness/", study_name, ".filter.smiss")) - 1` individuals removed.
3. Compute KING table (`--make-king-table`).
4. Identify duplicate individuals using kinship threshold > 0.35 as recommended by [PLINK](https://www.cog-genomics.org/plink/2.0/distance#make_king).
    ```{r duplicates2, echo = FALSE}
  t <- read.table(  paste0("analysis/QC/11_relatedness/", study_name, ".duplicates"))
  colnames(t) <- c('#FID1', 'ID1', 'FID2', 'ID2', 'NSNP', 'HETHET', 'IBS0', 'KINSHIP')
  kable(t) %>%
    kable_styling() %>%
    scroll_box(width = "100%")
    ```
    - A set of `r dim(t)[1]` duplicate particpants were found that were not expected.
    - These duplicates needed to be manually checked in database with Celine (see below).
5. Manually determine cause for duplicates.
    - These pairs of individuals were found to be in both studies under different IDs (PSYMETAB and Severines):

    ```{r eval=FALSE}
# 139CSM  HLHILECG
# 184CSM  WXPQPUWI
# 198CSM  CHMLAUKH
# 202CSM  OQLLRBMM
# 274UAS  YGGZRBYJ
# JO001 UUHDCMNL
# S008  YLXVRYDT
# S092  0021GE
    ```
    - These pairs of individuals are duplicates but with different codes and we are keeping the one on the right:
    ```{r eval=FALSE}
# ABDJGAXW  ZXDVAJUD
# JO280 JO276
# S093  JO426
    ```
    - These pair of individuals are not the same and the genetic info matches the one on the right (remove left):
    ```{r eval=FALSE}
# JNQSZGGL	XTVNZTRY
# WIELRZDD	FJNJEXCM
# OINDBCQM	SONUEGRB
    ```
    - These pair of individuals could not be properly sorted and are therefore both being deleted:
    ```{r eval=FALSE}
# S060	0062GE
# 0030GE	SJJCZOXT
    ```
6. Create file of duplicates to be removed (based on above criteria).
7. Remove duplicates.
    - `r countLines(paste0("analysis/QC/10_merge_imputed/", study_name, ".psam"))[1] - 1` individuals originally.
    - `r countLines(paste0("analysis/QC/11_relatedness/", study_name, ".duplicates2.FID_IID.txt"))[1]` individuals removed.
    - `r countLines(paste0("analysis/QC/11_relatedness/", study_name, "_remove_dups.psam")) - 1` individuals remaining.
8. Recalculate kinship and identify remaining related individauls using kinship threshold > 0.0884) as recommended by [KING authors](http://people.virginia.edu/~wc9c/KING/manual.html).
    - `#r countLines(paste0("analysis/QC/11_relatedness/", study_name, "2.related.filter.smiss"))[1] - 1` related pairs identified.
    - `#r countLines(paste0("analysis/QC/11_relatedness/", study_name, "2.duplicates"))[1] - 1` duplicate pairs identified.
9. Determine maximal set of unrelated individuals based on kinship results.
    - This was done using personal script (`code/qc/relatedness_filter.r`).
    - PLINK does not guarantee maximum number of samples (see [here](https://www.cog-genomics.org/plink/2.0/distance#make_king): *Relationship-based pruning*), where it states: *PLINK tries to maximize the final sample size, but this maximum independent set problem is NP-hard, so we use a greedy algorithm which does not guarantee an optimal result.*
10. Remove related individuals *(note that if using mixed model design we can include all the related IDs, but remove the problematic duplicates)*.
    - `r countLines(paste0("analysis/QC/11_relatedness/", study_name, "_related_ids.txt"))` related IDs removed.
    - `r countLines(paste0("analysis/QC/11_relatedness/", study_name, ".relatedness.step11.psam")) - 1` individuals remaining.

\newline

## 12. Ethnicty check and admixture estimation.

1. Clean typed data (from #9).
    - Remove duplicate samples (from #11).
    - Remove genotypes with missingness > 10% (using `--geno 0.1`).
    - Remove genotypes with MAF < 5% (using `--maf 0.05`).
    - Remove genotypes with HWE < 5e-4 (using `--hwe 5e-4`).
    - After filters, `r countLines(paste0("analysis/QC/12_ethnicity_admixture/", study_name, ".typed.QC.bim"))` SNPs remaining.
    - After filters, `r countLines(paste0("analysis/QC/12_ethnicity_admixture/", study_name, ".typed.QC.fam"))` individuals remaining.
2. Run [`snpweights`](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/181/2014/05/SNPweights2.1.tar.gz) software from Alkes Price [software page](https://www.hsph.harvard.edu/alkes-price/software/) using SNP weights for European, West African, East Asian and Native American ancestral populations (downloaded [here](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/181/2014/03/snpwt.NA_.zip)).
    - SNPweights is a software package for inferring genome-wide genetic ancestry using SNP weights precomputed from large external reference panels ([Chen et al. 2013 Bioinformatics](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3661048/)).
    - The software relies on external databases to infer ancestry weights, so related individuals can safely be included.
    - This software creates a file with the name `${output_name}.NA.predpc`.
    - The 10 columns in the `${output_name}.NA.predpc` output file are: `sample ID`, `population label`, `number of SNPs used for inference`, `predicted PC1`, `predicted PC2`, `predicted PC3`, `% YRI ancestry`, `% CEU ancestry`, `% East Asian ancestry`, and `% Native American ancestry`.
3. Filter to Europeans.
    - Using threshold `% CEU ancestry` > `0.8` as done in other papers.
    - `r countLines(paste0("analysis/QC/12_ethnicity_admixture/", study_name, ".typed.QC.fam")) - countLines(paste0("analysis/QC/12_ethnicity_admixture/snpweights/", study_name, ".CEU80"))` non-European individuals removed.
    - `r countLines(paste0("analysis/QC/12_ethnicity_admixture/snpweights/", study_name, ".CEU80"))` European individuals remaining.
4. Run `snpweights` in Europeans using SNP weights for NW, SE and AJ ancestral populations of European Americans (downloaded [here](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/181/2014/03/snpwt.EA_.zip)).
    - The 8 columns in the `${output_name}.CEU80.EA.predpc` output file are: `sample ID`, `population label`, `number of SNPs used for inference`, `predicted PC1`, `predicted PC2`, `% Northwest European ancestry`, `% Southeast European ancestry`, `% Ashkenazi Jewish ancestry`.
    - Note that nothing was done with these computed snpweights.
5. Using data from step 1, create pruned set of SNPs.
    - Remove related individuals (idenified in #11).
    - PLINK parameters set as: `--indep-pairwise 50 5 0.2`.
    - After pruning, `r countLines(paste0("analysis/QC/12_ethnicity_admixture/pca/", study_name, ".indep.prune.out"))` SNPs removed
    - Could consider excluding regions suggested by flaspca by adding the following line: `--exclude range $flashpca/exclusion_regions_hg19.txt`.
6. Create an unrelated dataset for calculating PCs using snps from step 5.
    - After pruning, `r countLines(paste0("analysis/QC/12_ethnicity_admixture/pca/", study_name, ".indep.prune.in"))` SNPs remaining.
    - `r countLines(paste0("analysis/QC/11_relatedness/", study_name, "_related_ids.txt"))` related samples removed.
    - `r countLines(paste0("analysis/QC/12_ethnicity_admixture/pca/", study_name, ".pruned.unrelated.fam"))` unrelated samples remaining.
7. Create a full set for projecting PCs with same set of pruned SNPs.
    - As above, `r countLines(paste0("analysis/QC/12_ethnicity_admixture/pca/", study_name, ".pruned.full.bim"))` SNPs remaining.
    - No samples removed, `r countLines(paste0("analysis/QC/12_ethnicity_admixture/pca/", study_name, ".pruned.full.fam"))` samples in total.
8. Perform PCA using [`flashpca`](https://github.com/gabraham/flashpca) in unrelated set (from step 6).
9. Perform PCA projections onto related sample set (from 12.7)
10. Split data into seperate ethnic groups as determined in 12.2.
    - For all ethnic groups except `YRI` (African), use a threshold of `% ancestry` > `0.8`.
    - For African, use a threshold of `% ancestry` > `0.7`.
    - If a participant has no ethnic group that meets this criteria (note that it is impossible to follow into multiple categories), they are classified as `MIXED`, in this way 5 ethnic groups remain: `CEU` (European), `EA` (East Asian), `NA` (Native American), `YRI` (West African), and `MIXED`.
```{r eth_count, include = FALSE}

eth_count <- numeric()
for (eth in c("CEU", "EA", "NA", "YRI", "MIXED")){

  temp <- countLines(paste0("analysis/QC/12_ethnicity_admixture/pca/",output_name, "_", eth, "_samples.txt"))[1]
  temp <- cbind(eth, temp)
  eth_count <- rbind(eth_count, temp)


}
total <- cbind("all", sum(as.numeric(as.character(eth_count[,2]))))
eth_count <- rbind(eth_count, total)
colnames(eth_count) <- c("Eth", "Sample size")
eth_count <- as.data.frame(eth_count)
eth_count


data_clean <- munge_snpweights(study_name, pc_data = read.table(paste0("analysis/QC/12_ethnicity_admixture/pca/", study_name, "_projections.txt"), header = T),
                               snp_weights = read.table(paste0("analysis/QC/12_ethnicity_admixture/snpweights/", study_name, ".NA.predpc"), header = F),
                               eth_file = "data/processed/phenotype_data/PSYMETAB_GWAS_eth.txt")
```

```{r eth_print, echo = FALSE}
tab <- table(data_clean$genetic_eth, data_clean$reported_eth)
tab <- cbind(tab, Total = rowSums(tab))
tab

### plot according to genetic eth
for(col_type in c("genetic", "reported"))
{


  #pdf(paste0(output_dir, "/", study_name,"_PCA_",col_type, "_eth.pdf"))
  out_plot <- pca_plot(data_clean, paste0(col_type,"_eth"), paste0(str_to_title(col_type), "Ancestry"),
                       paste0(study_name, " PCA according to ",col_type, " ancestry"))

  print(out_plot)
  #dev.off()

}

```

\newline

## 13. HWE check.

1. Using imputed data from #10, remove duplicates (from #11).
2. Split data into separate ethnicity files.
3. Within each ethnic group:
    - Remove related IDs (from #11).
    - If remaing n is `< 100` ignore.
    - If remaining n is `> 100`, evaluate HWE for each variant.
4. Identify SNPs failing HWE in at least one ethnic group.
5. Remove HWE violations (`p < 0.00000001`).
    - `r countLines(paste0("analysis/QC/13_hwecheck/", study_name, ".hardy.sig.unique"))` SNPs removed.
    - `r countLines(paste0("analysis/QC/13_hwecheck/", study_name, ".hwecheck.step13.pvar"))[1] - 14` SNPs remaining.

\newline

## 14. MAF check.
1. Using HWE filtered file (from #11 - which has duplicates removed), split data in separate ethnicity files.
2. Within each ethnic group with n > 100 (related individuals removed), calculate allele frequency.
3. Filter to only SNPs that have at least one MAF > threshold (0.01).
    - `r countLines(paste0("analysis/QC/14_mafcheck/", study_name, "_low_maf_snps.txt"))` SNPs removed.
    - `r countLines(paste0("analysis/QC/14_mafcheck/", study_name, ".mafcheck.step14.pvar"))[1] - 14` SNPs remaining.

\newline

# Step 6: Final processing.

**Source code for Step 5 can be found at: [code/final_processing.sh](https://github.com/jennysjaarda/PSYMETAB/blob/master/code/final_processing.sh).**

## 15. Final processing.

1. Seperate into ethnic groups with n > 100 and one file with all samples included (using data from #12).
2. Convert each ethnic subset and full file to bgen format (some programs require this format).
3. Convert each ethnic subset and full file to vcf format (some programs require this format).
4. Create imputation info file (could be used to further filter SNPs later).
5. Within each ethnic specific file, calculate final intra-ethnics PCs:
    1. Extract ethnicity samples from typed file with duplicates removed (from #12).
    2. Re-clean data within each ethnicity before final PCA (as before, using the followings flags: `--geno 0.1`, `--maf 0.05`, `--hwe 5e-4`).
    3. Prune, excluding related individuals (as before, using: `--indep-pairwise 50 5 0.2`).
        - PLINK2 will give an error if there are less than 50 samples to caclulate LD estimates. 
        - This can be overriden `--bad-ld`, but they suggest that *this is almost always a bad idea*. 
        - Of the five ethnic groups created in #12, the following groups did not proceed past this step due to insufficient sample size: 
    ```{r eth_size, echo = FALSE}
    
    for (eth in eths){
      pca_exists <- FALSE
      n <- NA
      if(file.exists(file.path(pc_dir, eth, paste0(study_name,"_",eth,"_projections.txt")))){
        pca_exists <- TRUE
        
      } else {
        cat("\n \n")
        cat(paste0("PCs were not successfully computed for ethnic group: ", eth, ". \n"))
        cat("The indep log file reads: \n \n")
        print(readLines(file.path(pc_dir, eth, paste0(study_name,".",eth,".indep.log"))))
        cat("\n--------------------------------------------------------------------------- \n")
      }
      
    }

    
    ```
    4. Create an unrelated set for calculating PCs with same set of pruned SNPs.
    5. Create a full set based on pruning in unrelated samples.
    6. Compute PCA components in unrelated set.
    7. Project onto full set.

*In general, clean, ethnic specific imputed data were used for analyses (from #15.1), and ethnic-specific PCs were used as adjustments (from #15.5).*

\newline

## 16. Compressing large intermediate files.

Run the following commands to compress some large intermediate files.

```{r, engine = 'bash', eval = FALSE}
project_dir="/data/sgg2/jenny/projects/PSYMETAB"
output_dir=$project_dir/analysis/QC

cd $output_dir/06_imputation_get
mkdir archive
tar --exclude='*.info.gz' --exclude qcreport.html --exclude snps-excluded.txt --exclude archive -czvf archive/06_imputation_get.tar.gz * --remove-files

cd $output_dir/10_merge_imputed
mkdir archive
tar --exclude='*.psam' --exclude='*.pvar' -czvf archive/10_merge_imputed.tar.gz * --remove-files

cd $output_dir/13_hwecheck
mkdir archive
tar --exclude='*.hardy.sig.unique' --exclude '*.hwecheck.step13.pvar' --exclude archive -czvf archive/13_hwecheck.tar.gz * --remove-files

cd $output_dir/14_mafcheck
mkdir archive
tar --exclude='*_low_maf_snps.txt' --exclude '*/*.afreq' --exclude '*.mafcheck.step14.pvar' --exclude archive -czvf archive/14_mafcheck.tar.gz * --remove-files

```

If you need to unzip these files, it can be done as follows: `tar xvzf file.tar.gz`

